# --------------------------------------------------------
# Parameter Freeze Log Hook
# Copyright (c) 2024
# Licensed under The MIT License [see LICENSE for details]
# --------------------------------------------------------

from mmcv.runner import HOOKS, Hook


@HOOKS.register_module()
class ParameterFreezeLogHook(Hook):
    """Hook to log frozen/trainable parameters at the beginning of training.
    
    Args:
        log_interval (int): Interval of iterations to print log. Default: 1.
            Set to 1 to only print at the first iteration.
    """
    
    def __init__(self, log_interval=1):
        self.log_interval = log_interval
        self.logged = False  # ç¡®ä¿åªæ‰“å°ä¸€æ¬¡
    
    def before_train_iter(self, runner):
        """Print parameter freeze status before the first training iteration."""
        if not self.logged and runner.iter % self.log_interval == 0:
            self._log_parameter_status(runner)
            self.logged = True
    
    def _log_parameter_status(self, runner):
        """Log the freeze status of model parameters."""
        model = runner.model
        if hasattr(model, 'module'):
            # For DistributedDataParallel
            model = model.module
        
        runner.logger.info("=" * 80)
        runner.logger.info("ğŸ” MODEL PARAMETER FREEZE STATUS")
        runner.logger.info("=" * 80)
        
        total_params = 0
        trainable_params = 0
        frozen_params = 0
        
        # ç»Ÿè®¡å„æ¨¡å—çš„å‚æ•°çŠ¶æ€
        module_stats = {}
        
        for name, param in model.named_parameters():
            total_params += param.numel()
            
            # è·å–æ¨¡å—åç§° (å–å‰ä¸¤çº§)
            module_name = '.'.join(name.split('.')[:2])
            
            if module_name not in module_stats:
                module_stats[module_name] = {
                    'total': 0,
                    'trainable': 0,
                    'frozen': 0,
                    'param_names': []
                }
            
            module_stats[module_name]['total'] += param.numel()
            
            if param.requires_grad:
                trainable_params += param.numel()
                module_stats[module_name]['trainable'] += param.numel()
                status = "âœ… TRAINABLE"
            else:
                frozen_params += param.numel()
                module_stats[module_name]['frozen'] += param.numel()
                status = "â„ï¸  FROZEN   "
            
            # è®°å½•å‚æ•°åç§°ï¼ˆåªè®°å½•å‰5ä¸ªï¼Œé¿å…æ—¥å¿—è¿‡é•¿ï¼‰
            if len(module_stats[module_name]['param_names']) < 5:
                module_stats[module_name]['param_names'].append((name, status, param.numel()))
        
        # æ‰“å°æ€»ä½“ç»Ÿè®¡
        runner.logger.info(f"ğŸ“Š OVERALL STATISTICS:")
        runner.logger.info(f"   Total Parameters:     {total_params:,}")
        runner.logger.info(f"   Trainable Parameters: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)")
        runner.logger.info(f"   Frozen Parameters:    {frozen_params:,} ({frozen_params/total_params*100:.1f}%)")
        runner.logger.info("")
        
        # æŒ‰æ¨¡å—æ‰“å°è¯¦ç»†ä¿¡æ¯
        runner.logger.info(f"ğŸ“‹ MODULE-WISE BREAKDOWN:")
        for module_name, stats in sorted(module_stats.items()):
            total_module = stats['total']
            trainable_module = stats['trainable']
            frozen_module = stats['frozen']
            
            status_icon = "âœ…" if trainable_module > 0 else "â„ï¸ "
            trainable_pct = trainable_module / total_module * 100 if total_module > 0 else 0
            
            runner.logger.info(f"   {status_icon} {module_name:<25} | "
                             f"Total: {total_module:>8,} | "
                             f"Trainable: {trainable_module:>8,} ({trainable_pct:>5.1f}%) | "
                             f"Frozen: {frozen_module:>8,}")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªå‚æ•°çš„è¯¦ç»†çŠ¶æ€
            for param_name, param_status, param_count in stats['param_names'][:3]:
                short_name = param_name.split('.')[-1]  # åªæ˜¾ç¤ºæœ€åä¸€çº§åç§°
                runner.logger.info(f"     â””â”€ {param_status} {short_name:<20} ({param_count:,} params)")
            
            if len(stats['param_names']) > 3:
                runner.logger.info(f"     â””â”€ ... and {len(stats['param_names'])-3} more parameters")
            
            runner.logger.info("")
        
        # æ‰“å°å…³é”®ä¿¡æ¯
        runner.logger.info("ğŸ¯ KEY INFORMATION:")
        
        # æ£€æŸ¥backboneæ˜¯å¦å†»ç»“
        backbone_frozen = all(not param.requires_grad 
                            for name, param in model.named_parameters() 
                            if name.startswith('backbone.'))
        
        if backbone_frozen:
            runner.logger.info("   ğŸ§Š Backbone is COMPLETELY FROZEN")
        else:
            runner.logger.info("   ğŸ”¥ Backbone contains TRAINABLE parameters")
        
        # æ£€æŸ¥decode_headæ˜¯å¦å¯è®­ç»ƒ
        decode_head_trainable = any(param.requires_grad 
                                  for name, param in model.named_parameters() 
                                  if name.startswith('decode_head.'))
        
        if decode_head_trainable:
            runner.logger.info("   ğŸ¯ Decode Head is TRAINABLE")
        else:
            runner.logger.info("   â„ï¸  Decode Head is FROZEN")
        
        runner.logger.info("=" * 80) 