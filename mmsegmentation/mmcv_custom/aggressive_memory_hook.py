"""
æ¿€è¿›å†…å­˜æ¸…ç†Hook - ä¸“é—¨ç”¨äºè§£å†³è¯„æµ‹æ—¶OOMé—®é¢˜
"""
import torch
import gc
import psutil
import os
from mmcv.runner import HOOKS, Hook


@HOOKS.register_module()
class AggressiveMemoryHook(Hook):
    """æ¿€è¿›çš„å†…å­˜æ¸…ç†Hookï¼Œç”¨äºè¯„æµ‹æ—¶è§£å†³OOMé—®é¢˜"""
    
    def __init__(self, 
                 cleanup_interval=1, 
                 verbose=True,
                 force_gc=True,
                 reset_stats=True,
                 clear_grad=True):
        """
        Args:
            cleanup_interval (int): æ¯éš”å¤šå°‘ä¸ªiterationæ¸…ç†ä¸€æ¬¡å†…å­˜
            verbose (bool): æ˜¯å¦æ‰“å°å†…å­˜ä½¿ç”¨æƒ…å†µ
            force_gc (bool): æ˜¯å¦å¼ºåˆ¶åƒåœ¾å›æ”¶
            reset_stats (bool): æ˜¯å¦é‡ç½®å†…å­˜ç»Ÿè®¡
            clear_grad (bool): æ˜¯å¦æ¸…ç†æ¢¯åº¦
        """
        self.cleanup_interval = cleanup_interval
        self.verbose = verbose
        self.force_gc = force_gc
        self.reset_stats = reset_stats
        self.clear_grad = clear_grad
    
    def _log_memory_info(self, stage=""):
        """è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if not self.verbose:
            return
            
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            max_allocated = torch.cuda.max_memory_allocated() / 1024**3
            print(f"ğŸ§  [{stage}] GPU Memory: {allocated:.2f}GB allocated, "
                  f"{reserved:.2f}GB reserved, {max_allocated:.2f}GB max")
        
        # ç³»ç»Ÿå†…å­˜
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        print(f"ğŸ’¾ [{stage}] RAM: {memory_info.rss / 1024**3:.2f}GB")
    
    def _aggressive_cleanup(self):
        """æ¿€è¿›çš„å†…å­˜æ¸…ç†"""
        if torch.cuda.is_available():
            # æ¸…ç†æ¢¯åº¦
            if self.clear_grad:
                for param in torch.cuda._C._get_allocator_backend()._get_allocator_state():
                    if hasattr(param, 'grad') and param.grad is not None:
                        param.grad.detach_()
                        param.grad = None
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            if self.force_gc:
                gc.collect()
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            # é‡ç½®å†…å­˜ç»Ÿè®¡
            if self.reset_stats:
                if hasattr(torch.cuda, 'reset_peak_memory_stats'):
                    torch.cuda.reset_peak_memory_stats()
                if hasattr(torch.cuda, 'reset_accumulated_memory_stats'):
                    torch.cuda.reset_accumulated_memory_stats()
            
            # å°è¯•é‡Šæ”¾æ›´å¤šå†…å­˜
            torch.cuda.empty_cache()
    
    def before_run(self, runner):
        """è¿è¡Œå¼€å§‹å‰æ¸…ç†å†…å­˜"""
        self._log_memory_info("Before Run")
        self._aggressive_cleanup()
        self._log_memory_info("After Cleanup")
    
    def before_epoch(self, runner):
        """æ¯ä¸ªepochå¼€å§‹å‰æ¸…ç†å†…å­˜"""
        self._log_memory_info("Before Epoch")
        self._aggressive_cleanup()
    
    def after_epoch(self, runner):
        """æ¯ä¸ªepochç»“æŸåæ¸…ç†å†…å­˜"""
        self._log_memory_info("After Epoch")
        self._aggressive_cleanup()
    
    def before_iter(self, runner):
        """æ¯ä¸ªiterationå¼€å§‹å‰æ¸…ç†å†…å­˜"""
        if runner._iter % self.cleanup_interval == 0:
            self._aggressive_cleanup()
    
    def after_iter(self, runner):
        """æ¯ä¸ªiterationç»“æŸåæ¸…ç†å†…å­˜"""
        if runner._iter % self.cleanup_interval == 0:
            self._log_memory_info(f"Iter {runner._iter}")
            self._aggressive_cleanup()
            self._log_memory_info(f"After Iter {runner._iter}")
    
    def before_val_epoch(self, runner):
        """éªŒè¯å¼€å§‹å‰æ¸…ç†å†…å­˜"""
        self._log_memory_info("Before Validation")
        self._aggressive_cleanup()
    
    def after_val_epoch(self, runner):
        """éªŒè¯ç»“æŸåæ¸…ç†å†…å­˜"""
        self._log_memory_info("After Validation")
        self._aggressive_cleanup()
    
    def before_test_epoch(self, runner):
        """æµ‹è¯•å¼€å§‹å‰æ¸…ç†å†…å­˜"""
        self._log_memory_info("Before Test")
        self._aggressive_cleanup()
    
    def after_test_epoch(self, runner):
        """æµ‹è¯•ç»“æŸåæ¸…ç†å†…å­˜"""
        self._log_memory_info("After Test")
        self._aggressive_cleanup()
