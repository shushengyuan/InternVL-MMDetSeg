"""
GPUå†…å­˜æ¸…ç†Hook - ç”¨äºåœ¨è®­ç»ƒåæ¸…ç†GPUå†…å­˜ä»¥é¿å…è¯„ä¼°æ—¶OOM
"""
import torch
from mmcv.runner import HOOKS, Hook


@HOOKS.register_module()
class GPUMemoryCleanupHook(Hook):
    """åœ¨è®­ç»ƒåæ¸…ç†GPUå†…å­˜çš„Hook"""
    
    def __init__(self, cleanup_interval=1, verbose=True):
        """
        Args:
            cleanup_interval (int): æ¯éš”å¤šå°‘ä¸ªiterationæ¸…ç†ä¸€æ¬¡å†…å­˜
            verbose (bool): æ˜¯å¦æ‰“å°å†…å­˜ä½¿ç”¨æƒ…å†µ
        """
        self.cleanup_interval = cleanup_interval
        self.verbose = verbose
    
    def after_train_iter(self, runner):
        """åœ¨æ¯æ¬¡è®­ç»ƒiterationåæ¸…ç†GPUå†…å­˜"""
        if runner._iter % self.cleanup_interval == 0:
            if torch.cuda.is_available():
                if self.verbose:
                    allocated_before = torch.cuda.memory_allocated() / 1024**3
                    reserved_before = torch.cuda.memory_reserved() / 1024**3
                
                # æ¸…ç†æ¢¯åº¦
                if hasattr(runner, 'optimizer') and runner.optimizer is not None:
                    runner.optimizer.zero_grad()
                
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                import gc
                gc.collect()
                
                # æ¸…ç†CUDAç¼“å­˜
                torch.cuda.empty_cache()
                
                if self.verbose:
                    allocated_after = torch.cuda.memory_allocated() / 1024**3
                    reserved_after = torch.cuda.memory_reserved() / 1024**3
                    # print(f"ğŸ§¹ [MemoryCleanup] Iter {runner._iter}: "
                    #       f"{allocated_before:.2f}â†’{allocated_after:.2f} GB allocated, "
                    #       f"{reserved_before:.2f}â†’{reserved_after:.2f} GB reserved")
    
    def before_val_epoch(self, runner):
        """åœ¨éªŒè¯å¼€å§‹å‰å¼ºåˆ¶æ¸…ç†å†…å­˜"""
        if torch.cuda.is_available():
            if self.verbose:
                allocated_before = torch.cuda.memory_allocated() / 1024**3
                reserved_before = torch.cuda.memory_reserved() / 1024**3
            
            # å¼ºåˆ¶æ¸…ç†æ‰€æœ‰ç¼“å­˜
            import gc
            gc.collect()
            
            # æ›´æ¿€è¿›çš„å†…å­˜æ¸…ç†
            torch.cuda.empty_cache()
            torch.cuda.synchronize()  # ç­‰å¾…æ‰€æœ‰CUDAæ“ä½œå®Œæˆ
            
            # å°è¯•é‡Šæ”¾æ›´å¤šå†…å­˜
            if hasattr(torch.cuda, 'reset_peak_memory_stats'):
                torch.cuda.reset_peak_memory_stats()
            
            if self.verbose:
                allocated_after = torch.cuda.memory_allocated() / 1024**3
                reserved_after = torch.cuda.memory_reserved() / 1024**3
                print(f"ğŸ§¹ [MemoryCleanup] Before validation: "
                      f"{allocated_before:.2f}â†’{allocated_after:.2f} GB allocated, "
                      f"{reserved_before:.2f}â†’{reserved_after:.2f} GB reserved")
            
            # å¼ºåˆ¶æ¨¡å‹è¿›å…¥evalæ¨¡å¼å¹¶æ¸…ç†è®­ç»ƒçŠ¶æ€
            if hasattr(runner, 'model') and runner.model is not None:
                runner.model.eval()
                # æ¸…ç†å¯èƒ½çš„æ¢¯åº¦ç¼“å­˜
                for param in runner.model.parameters():
                    if param.grad is not None:
                        param.grad.detach_()
                        param.grad = None
